# -*- coding: utf-8 -*-
"""BuySafe.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aPCfLboVOBa4yB65zu40fVgSa4SXrrJv
"""



import pandas as pd
import numpy as np
import joblib
import requests
from bs4 import BeautifulSoup
import ssl
import socket
import whois
from urllib.parse import urlparse
import re
from datetime import datetime
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from imblearn.over_sampling import SMOTE

def extract_features(url):
    features = {}

    # Ensure URL has scheme
    if not url.startswith("http"):
        url = "https://" + url

    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        response = requests.get(url, headers=headers, timeout=5)
        html = response.text
        soup = BeautifulSoup(html, "html.parser")
        text = soup.get_text().lower()
    except Exception as e:
        print("Website fetch error:", e)
        return None

    # -------------------------
    # CONTENT FEATURES
    # -------------------------
    features["refund_policy"] = "refund" in text
    features["contact_details"] = "contact" in text or "email" in text or "phone" in text

    suspicious_words = ["lottery", "free money", "win cash", "100% guarantee", "risk free"]
    features["suspicious_words"] = any(word in text for word in suspicious_words)

    fake_discount_patterns = re.findall(r"\d{2,3}%\s*off", text)
    features["fake_discounts_detected"] = len(fake_discount_patterns) > 0

    features["product_info_present"] = "product" in text or "add to cart" in text

    # -------------------------
    # DOMAIN FEATURES (WHOIS)
    # -------------------------
    try:
        domain_info = whois.whois(domain)

        creation_date = domain_info.creation_date
        expiry_date = domain_info.expiration_date

        if isinstance(creation_date, list):
            creation_date = creation_date[0]
        if isinstance(expiry_date, list):
            expiry_date = expiry_date[0]

        features["creation_date"] = str(creation_date)
        features["expiry_date"] = str(expiry_date)

        features["owner_hidden"] = "privacy" in str(domain_info).lower()

    except Exception:
        features["creation_date"] = None
        features["expiry_date"] = None
        features["owner_hidden"] = None

    # -------------------------
    # SSL FEATURES
    # -------------------------
    try:
        context = ssl.create_default_context()
        with socket.create_connection((domain, 443), timeout=5) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                features["ssl_exists"] = True
                features["ssl_valid"] = True
                features["encryption_version"] = ssock.version()
    except Exception:
        features["ssl_exists"] = False
        features["ssl_valid"] = False
        features["encryption_version"] = None

    # -------------------------
    # URL STRUCTURE FEATURES
    # -------------------------
    features["uses_https"] = url.startswith("https")
    features["url_length"] = len(url)
    features["has_ip_address"] = bool(re.match(r"^\d+\.\d+\.\d+\.\d+$", domain))
    features["subdomain_count"] = domain.count(".") - 1

    return features

# Generate synthetic dataset
np.random.seed(42)
n_samples = 500
data = []

for _ in range(n_samples):
    https = np.random.randint(0,2)
    ssl_valid = np.random.randint(0,2)
    refund = np.random.randint(0,2)
    contact = np.random.randint(0,2)
    owner_hidden = np.random.randint(0,2)
    suspicious_words = np.random.randint(0,2)
    fake_discount = np.random.randint(0,2)
    domain_age_days = np.random.randint(1,2000)

    fraud_score = owner_hidden + suspicious_words + fake_discount
    if domain_age_days < 90:
        fraud_score += 1
    if https == 0:
        fraud_score += 1

    label = 1 if fraud_score >= 3 else 0
    data.append([https, ssl_valid, refund, contact, owner_hidden, suspicious_words, fake_discount, domain_age_days, label])

columns = ["https","ssl_valid","refund","contact","owner_hidden","suspicious_words","fake_discount","domain_age_days","label"]
df = pd.DataFrame(data, columns=columns)
df.to_csv("dataset.csv", index=False)

print("Dataset created!", df.shape)
print("Class distribution:\n", df['label'].value_counts())

# Train model

df = pd.read_csv("dataset.csv")
X = df.drop("label", axis=1)
y = df["label"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

param_grid = {"n_estimators":[100,200], "max_depth":[None,10,20], "min_samples_split":[2,5]}
grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring="f1", n_jobs=-1)
grid.fit(X_train, y_train)
model = grid.best_estimator_

print("Best Parameters:", grid.best_params_)

preds = model.predict(X_test)
probs = model.predict_proba(X_test)[:,1]
print(classification_report(y_test, preds))
print("ROC-AUC Score:", roc_auc_score(y_test, probs))

joblib.dump(model, "fraud_model.pkl")
print("Model saved as fraud_model.pkl")

from datetime import datetime

def safe_int(value):
    if value is None:
        return 0
    return int(bool(value))


def convert_to_model_features(raw):

    # ---- DOMAIN AGE ----
    domain_age_days = 365
    if raw.get("creation_date"):
        try:
            created = datetime.strptime(str(raw["creation_date"])[:10], "%Y-%m-%d")
            domain_age_days = (datetime.now() - created).days
        except:
            domain_age_days = 365

    model_features = {
        "https": safe_int(raw.get("uses_https")),
        "ssl_valid": safe_int(raw.get("ssl_valid")),
        "refund": safe_int(raw.get("refund_policy")),
        "contact": safe_int(raw.get("contact_details")),
        "owner_hidden": safe_int(raw.get("owner_hidden")),
        "suspicious_words": safe_int(raw.get("suspicious_words")),
        "fake_discount": safe_int(raw.get("fake_discounts_detected")),
        "domain_age_days": int(domain_age_days)
    }

    return model_features

# Prediction

model = joblib.load("fraud_model.pkl")

def predict_site(raw_features):

    # convert extracted features â†’ model features
    features = convert_to_model_features(raw_features)

    feature_array = np.array([[
        features["https"],
        features["ssl_valid"],
        features["refund"],
        features["contact"],
        features["owner_hidden"],
        features["suspicious_words"],
        features["fake_discount"],
        features["domain_age_days"]
    ]])

    fraud_prob = model.predict_proba(feature_array)[0][1]
    trust_score = int((1 - fraud_prob) * 100)

    # risk label
    if trust_score >= 80:
        risk = "Safe"
    elif trust_score >= 50:
        risk = "Moderate Risk"
    else:
        risk = "High Risk"

    # explainability (reasons)
    reasons = []
    if not features["https"]:
        reasons.append("No HTTPS")
    if not features["ssl_valid"]:
        reasons.append("Invalid SSL certificate")
    if features["suspicious_words"]:
        reasons.append("Suspicious words detected")
    if features["fake_discount"]:
        reasons.append("Fake discount detected")
    if features["owner_hidden"]:
        reasons.append("Owner identity hidden")
    if features["domain_age_days"] < 90:
        reasons.append("Newly created domain")

    return trust_score, risk, reasons
